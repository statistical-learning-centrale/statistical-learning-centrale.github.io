<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">


  <!-- Google Analytics -->
  <!-- bootstrap -->
  <link rel="stylesheet" href="./stylesheets/main/bootstrap.min.css">
  <link rel="stylesheet" href="./stylesheets/main/bootstrap-theme.min.css">

  <!-- Google fonts -->
  <link href="./stylesheets/main/css" rel="stylesheet" type="text/css">

  <style type="text/css"></style>

  <link rel="stylesheet" type="text/css" href="./stylesheets/main/style.css">

</head>

<body onload="page_loaded()">

<div id="header">
  <a href="http://math.univ-lyon1.fr//">
    <img src="./stylesheets/main/icj.png" style="height:50px; float: left; margin-left: 20px;">
  </a>
  <a href="https://www.ec-lyon.fr/">
    <img src="./stylesheets/main/ecl.png" style="height:50px; float: right; margin-right: 20px;">
  </a>
    <h1>Advanced Tools for Learning</h1>
  <div style="clear:both;"></div>
</div>

<!-- <div class="sechighlight">-->
  <div class="container sec">
      <h4>Course ID ENS Lyon: Inverse problems and high dimension - <b>MATH5212</b><h4>
      <h4>Course ID ECL: S9 Advanced Tools for Learning : when Convexity meets Sparsity - <b>S9 MD fo IM3.1</b><h4>
</div>
<!-- </div>-->


<div class="sechighlight">
  <div class="container sec">
    <h2>Course Description</h2>

    <div id="coursedesc">
      Sparsity and convexity are ubiquitous notions in Machine Learning and Statistics.
      In this course, we study the mathematical foundations of some powerful methods based on convex relaxation:
      L1-regularisation techniques in Statistics and Signal Processing;
      Nuclear Norm minimization in Matrix Completion.
      These approaches turned to be Semi-Definite representable (SDP) and hence tractable in practice.
      The theoretical part of the course will focus on the guarantees of these algorithms under the sparsity assumption.
      The practical part of this course will present the standard solvers of these learning problems.<br>
      Keywords: L1-regularisation; Matrix Completion; Semi-Definite Programming; Proximal methods;
 </div>
    <br>
    <div>
    "Nothing is more practical than a good theory." - V. Vapnik
    </div>

   <h2>Bibliography</h2>
   <div>
   <ul>
     <li>
       <b>[Giraud] </b> Introduction to High-Dimensional Statistics, Christophe Giraud, Chapman and Hall/CRC
     </li>
     <li>
       <b>[Wainwright] </b> High-Dimensional Statistics: A Non-Asymptotic Viewpoint, Martin J. Wainwright, Cambridge University Press
     </li>
     <li> <b>[Foucart] </b> A Mathematical Introduction to Compressive Sensing, Simon Foucart and Holger Rauhut.
     </li>
     <li>
       <b>[Le Gall] </b> <a href="https://www.imo.universite-paris-saclay.fr/~jflegall/IPPA2.pdf">Intégration, Probabilités et Processus Aléatoires (PDF, 248 pages)</a>
     </li>
   </ul>
   </div>
</div>
</div>

<div class="container sec">
  <div class="row">
    <div> <!-- class="col-md-4">-->
      <h2>Course Organizers</h2>
      <div class="instructor">
        <a href="https://ydecastro.github.io/">
        <div class="instructorphoto"><img src="./stylesheets/main/yohann.png"></div>
        <div>Yohann <br>De Castro</div>
        </a>
      </div>
      <div class="instructor">
        <a href="https://people.irisa.fr/Remi.Gribonval/">
      <div class="instructorphoto"><img src="./stylesheets/main/remi.png"></div>
        <div>Rémi <br>Gribonval</div>
        </a>
      </div>
      <h2> Announcements </h2>
          <div id="coursedesc">
              Due to the sanitary situation, this course is online otherwise specified. Please follow the link below to join the course on Zoom.
          </div>
      <h3> Projects </h3>
      The guidelines can be found in <a href="./doc/evaluation_s10.pdf">this document</a>.
      Please send an email to both lecturers with your group (4 names) and your ordered list of three subjects for the 26th of Jannuary.
      <div id="col-md-6">
        <ul>
          <li>[1] Beck, A., & Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems.
SIAM Journal on Imaging Sciences, 2(1), 183–202. http://doi.org/10.1137/080716542
          </li>[2] E. Ndiaye, O. Fercoq, A. Gramfort, and J. Salmon, “Gap Safe Screening Rules for Sparsity Enforcing Penalties.,”
J. Mach. Learn. Res., 2017.
          <li>[3] M. Kabanava, R. Kueng, and H. Rauhut, “Stable low-rank matrix recovery via null space properties,”
Information and Inference, vol. 5, no. 4, pp. 405–441, 2016.
          </li>[4] Q. Denoyelle, V. Duval, G. Peyré, and E. Soubies, “The sliding Frank–Wolfe algorithm and its application to super-resolution microscopy,”
Inverse problems, vol. 36, no. 1, p. 014001, Jan. 2020.
          <li>[5] Bouwmans, T., & Zahzah, E.-H. (2014). Robust PCA via Principal Component Pursuit - A review for a comparative evaluation in video surveillance.
Computer Vision and Image Understanding, 122, 22–34. http://doi.org/10.1016/j.cviu.2013.11.009
          </li>
          <li>[6] J. Yang, J. Wright, T. Huang, and Y. Ma, Image Super-Resolution Via Sparse Representation,
IEEE Trans. Image Process., vol. 19, no. 11, pp. 2861 –2873, November 2010. http://doi.org/10.1109/TIP.2010.2050625
          </li>
          <li>[7] Mairal, J., Elad, M., & Sapiro, G. (2008). Sparse Representation for Color Image Restoration.
IEEE Transactions on Image Processing, 17(1), 53–69. http://doi.org/10.1109/tip.2007.911828
          </li>
          <li>[8] Patrik O. Hoyer. Non-negative Matrix Factorization with Sparseness Constraints.
            Journal of machine learning research, 5(Nov):1457--1469, 2004.
            https://www.jmlr.org/papers/volume5/hoyer04a/hoyer04a.pdf
          </li>
          <li>[9] Jacob, L., Obozinski, G., & Vert, J. P. (2009, June). Group lasso with overlap and graph lasso.
            In Proceedings of the 26th annual international conference on machine learning (pp. 433-440).
            http://members.cbio.mines-paristech.fr/~ljacob/documents/overlasso.pdf
          </li>
          <li>[10] Loh, P. L., & Wainwright, M. J. (2011).
            High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity.
            In Advances in Neural Information Processing Systems (pp. 2726-2734).
            https://papers.nips.cc/paper/2011/hash/ab541d874c7bc19ab77642849e02b89f-Abstract.html
          </li>
          <li>[11] Berthet, Q., & Rigollet, P. (2013).
            Optimal detection of sparse principal components in high dimension.
            The Annals of Statistics, 41(4), 1780-1815.
          </li>
          <li>[12] Vladimir Koltchinskii, Alexandre B. Tsybakov, Karim Lounici.
            Nuclear norm penalization and optimal rates for noisy low rank matrix completion.
            The Annals of Statistics, 39(5), 2302-2329. https://arxiv.org/pdf/1011.6256.pdf
          </li>
          <li>[13] Two papers on Sketching:<br>
            Antoine Chatalic, Rémi Gribonval, Nicolas Keriven. Large-Scale High-Dimensional Clustering with Fast Sketching . In ICASSP, 2018. <br>
            Rémi Gribonval, Antoine Chatalic, Nicolas Keriven, Vincent Schellekens, Laurent Jacques, Philip Schniter. Sketching Datasets for Large-Scale Learning (long version). arXiv preprint arXiv:2008.01839.
          </li>
        </ul>
      </div>
      </div>
  </div>
</div>

<div class="sechighlight">
<div class="container sec">
  <h2>Videos Links</h2>

  <b>Yohann:</b> <a href="https://ec-lyon-fr.zoom.us/j/3399373722"> Zoom link to Yohann's</a><url> <br><br>
    <b>Rémi:</b> <a href=" https://etudes.ens-lyon.fr/mod/bigbluebuttonbn/view.php?id=105390"> BBB link to Remi's (be sure to have an account on ENSL)</a><url> <br><br>
</div>
</div>
<!--
<div class="sechighlight">
  <div style="text-align:center; padding:40px 0px 40px 0px;">
    <a href="https://docs.google.com/spreadsheets/d/112cJrHd8FPNv3iRFSCI0fnxueubd51xg9hZ5Lt1z0LU/edit?usp=sharing" style="margin-right:50px">
      <button type="button" class="btn btn-success btn-lg">Key Papers</button>
    </a>
    <a href="./computer_learning_syllabus.pdf">
      <button type="button" class="btn btn-warning btn-lg">Detailed Syllabus</button>
    </a>
  </div>
</div>
-->

<div class="container sec">
  <div class="row">
    <div class="col-md-6">
      <h2>Class Time and Location</h2>
      Winter quarter (January - March, 2021).<br>
      Lecture: Tuesday afternoon 2:00-5:00 or 6:00<br>
      Ecole Centrale Lyon, <b>Building W1</b>, <b>Room 101</b> (second floor)<br>

    </div>
    <div class="col-md-4">
      <h2>Contact Info</h2>
      Yohann: yohann.de-castro@ec-lyon.fr<br>
      Rémi: remi.gribonval@inria.fr <br>
    </div>
  </div>
</div>

<body>

</div>

<!--
<div class="sechighlight">
<div class="container sec">
  <h2>Schedule</h2>
  <ul>
    <li><h4>Week 1: 5th of January (by Yohann) from 2pm to 6pm</h4>
      <b>S9 MD fo IM3.1</b> -> ECL students only<br>
        Probabilty toolbox
        </li>
    <li><h4>Week 2: 12th of January (by Yohann) from 2pm to 6pm</h4>
          <b>S9 MD fo IM3.1</b> -> ECL students only<br>
        Optimization toolbox
        </li>
    <li><h4>Week 3: 19th of January (by Rémi) from 2pm to 5pm</h4>
          <b>S9 MD fo IM3.1</b> and <b>MATH5213</b> -> Full Group<br>
          Lecture 1
        </li>
    <li><h4>Week 4: 26th of January (by Rémi) from 2pm to 5pm</h4>
      <b>S9 MD fo IM3.1</b> and <b>MATH5213</b> -> Full Group<br>
          Lecture 2
        </li>
    <li><h4>Week 5: 2nd of February (by Yohann) from 2pm to 6pm</h4>
            <b>S9 MD fo IM3.1</b> -> ECL students only<br>
        Stastistical Learning toolbox
        </li>
    <li><h4>Week 6: 9th of February (by Yohann) from 2pm to 5pm</h4>
          <b>S9 MD fo IM3.1</b> and <b>MATH5213</b> -> Full Group<br>
          Lecture 3
        </li>
    <li><h4> BREAK on 16th of February</h4>
    </li>
    <li><h4>Week 7: 26th of February (by Rémi) from 2pm to 5pm</h4>
            <b>S9 MD fo IM3.1</b> and <b>MATH5213</b> -> Full Group<br>
            Lecture 4
    </li>
    <li><h4>Week 8: 2nd of March (by Rémi) from 2pm to 5pm</h4>
            <b>S9 MD fo IM3.1</b> and <b>MATH5213</b> -> Full Group<br>
            Lecture 5 + 1h of project presentation by each group
    </li>
    <li><h4>Week 9: 9th of March (by Yohann) from 2pm to 5pm</h4>
            <b>MATH5213</b> -> ENSL Student only<br>
            Lecture 6
    </li>
    <li><h4>Week 10: 16th of March  from 2pm to 5pm</h4>
            <b>S9 MD fo IM3.1</b> and <b>MATH5213</b> -> Full Group<br>
            Project Evaluation
    </li>
    <li><h4>Week 11: 23th of March  from 2pm to 4pm</h4>
            <b>S9 MD fo IM3.1</b> and <b>MATH5213</b> -> Full Group<br>
            Final Examination
    </li>
  </ul>
</div>
</div>
-->
<div class="sechighlight">
 <div class="container sec">
<table class="table">
  <tbody><tr class="active">
    <th>Students involved</th><th>Date and time</th><th>Lecturer</th><th>Description</th><th>Materials and references</th>
  </tr>
  <tr>
    <td><b>S9 MD fo IM3.1</b> <br>-> ECL students only</td>
    <td>Jan., 5 <br>(2-6pm)</td>
    <td>Yohann</td>
    <td>Probabilty toolbox</td>
    <td>
      <b>[Giraud; P.17, 22, 24, 219, 221, 223]</b>
      <b>[Wainwright; Chap. 2]</b>
      <a href="https://www.dropbox.com/s/7ir3zkz1afym6gu/zoom_course1_S10_2021.mp4?dl=1">Video 1 </a>
      <a href="https://www.dropbox.com/s/881se6e6z5j4mk0/zoom_course1_S10_2021b.mp4?dl=1">Video 2 </a>
      <a href="./notes/Notes de Yohann du 3 janv. 2022.pdf">Notes</a>
    </td>
  </tr>
  <tr>
    <td><b>S9 MD fo IM3.1</b> <br>-> ECL students only</td>
    <td>Jan., 12 <br>(2-6pm)</td>
    <td>Yohann</td>
    <td>Optimization toolbox</td>
    <td>
      <b>[Foucart; App. B]</b>
      <a href="https://www.dropbox.com/s/phj23d3nbwki1lu/zoom_course2.mp4?dl=1">Video 1 </a>
      <a href="./notes/Notes de Yohann du 10 janv. 2023.pdf">Notes</a>
    </td>
  </tr>
  <tr>
    <td><b>S9 MD fo IM3.1</b> and <b>MATH5213</b> <br>-> Full group</td>
    <td>Jan., 19 <br>(2-5pm)</td>
    <td>Rémi</td>
    <td>Lecture 1</td>
    <td></td>
  </tr>
  <tr>
    <td><b>S9 MD fo IM3.1</b> and <b>MATH5213</b> <br>-> Full group</td>
    <td>Jan., 26 <br>(2-5pm)</td>
    <td>Rémi</td>
    <td>Lecture 2</td>
    <td></td>
  </tr>
  <tr>
    <td><b>S9 MD fo IM3.1</b> <br>-> ECL students only</td>
    <td>Feb., 2 <br>(2-6pm)</td>
    <td>Yohann</td>
    <td>Statistical Learning toolbox</td>
    <td></td>
  </tr>
  <tr>
    <td><b>S9 MD fo IM3.1</b> and <b>MATH5213</b> <br>-> Full group<br></td>
    <td>Feb., 9 <br>(2-5pm)</td>
    <td>Yohann</td>
    <td>Lecture 3</td>
    <td></td>
  </tr>
  <tr>
    <td>None</td>
    <td>Feb., 16</td>
    <td>None</td>
    <td>BREAK</td>
    <td></td>
  </tr>
  <tr>
    <td><b>S9 MD fo IM3.1</b> and <b>MATH5213</b> <br>-> Full group</td>
    <td>Feb., 23 <br>(2-5pm)</td>
    <td>Yohann</td>
    <td>Lecture 4</td>
    <td></td>
  </tr>
  <tr>
    <td><b>S9 MD fo IM3.1</b> and <b>MATH5213</b> <br>-> Full group</td>
    <td>March, 2 <br>(2-5pm)</td>
    <td>Rémi</td>
    <td>Lecture 5 + 1h of project presentation by each group (training)</td>
    <td></td>
  </tr>
  <tr>
    <td><b>MATH5213</b> <br>-> ENSL students only</td>
    <td>March, 9 <br>(2-5pm)</td>
    <td>Rémi</td>
    <td>Lecture 6</td>
    <td></td>
  </tr>
  <tr>
    <td><b>S9 MD fo IM3.1</b> and <b>MATH5213</b> <br>-> Full group<br></td>
    <td>March, 16 <br>(2-5pm)</td>
    <td>Examinators: Rémi and Yohann</td>
    <td>Project Evaluation</td>
    <td></td>
  </tr>
  <tr>
    <td><b>S9 MD fo IM3.1</b> and <b>MATH5213</b> <br>-> Full group</td>
    <td>March, 23 <br>(2-4pm)</td>
    <td>Examinator: Yohann</td>
    <td>Final Examination</td>
    <td></td>
  </tr>
</tbody></table>
</div>
</div>

<!-- jQuery and Boostrap -->
<script src="./stylesheets/jquery.min.js"></script>
<script src="./stylesheets/bootstrap.min.js"></script>



</body></html>


  </body>
</html>
